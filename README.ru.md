# Snake-With-Q-Learning (RU)

Интерактивная игра «Змейка» с эволюционным Q‑обучением. Рендер через `pixels`/`winit` с полупрозрачной панелью управления. Можно играть вручную или наблюдать, как популяция агентов обучается играть с помощью Q‑learning и эволюционных стратегий.

[English version](./README.md)

## Возможности

- Классическая «Змейка» на фиксированной сетке (окно 800×600, ячейки по 20 пикселей) с тороидальным переходом через края (wrap-around).
- Плавный пиксельный рендер с шахматным фоном и «глазами» у головы змеи.
- Экранная панель: счёт, длина, скорость, статус эволюции, график лучших результатов по эпохам, быстрые кнопки.
- Q‑агент с компактным представлением состояния (20 бит) и тремя действиями: повернуть влево, прямо, вправо.
- Эволюционный тренер (популяция по умолчанию 24): параллельный запуск нескольких агентов, элитизм, мутации, адаптивные рестарты при стагнации. Встроена «защита лидера» — уникально лучший агент может продолжать шаги сверх лимита эпохи.
- [Отключено] Автосохранение/автозагрузка табличного чемпиона в `snake_agent.json`.
- DQN (Candle) сохраняет веса в `dqn_agent.safetensors` (нажмите S или выключите J при активном DQN).
- Учёт доступности GPU: при наличии адаптера повышается бюджет шагов/тик; есть клавиша для переключения.

## Управление

  - Ручная игра: `+`/`-` изменяют длительность тика
  - Эволюция: `+` удваивает и `-` делит на 2 шаги/кадр (до 100 000)

### Модель для NPU (DirectML)

При включении NPU-режима (клавиша K) приложение ищет ONNX-модель (по умолчанию `snake_dqn.onnx`). Можно:

- Указать путь через переменную окружения `SNAKE_NPU_ONNX`, или
- Положить `snake_dqn.onnx` в одну из папок рядом с исполняемым файлом:
  - `./`
  - `models/`
  - `assets/`
  - `target/release/` или `target/debug/`

Если модель не найдена, в консоли будет подсказка с инструкциями.
## Сборка и запуск

Требуется:
- Установленный Rust (stable)
- Windows (проверено), также должно работать на других платформах, поддерживаемых `pixels`/`winit`.

Запуск (debug):

```powershell
cargo run
```

Запуск (оптимизированный):

```powershell
cargo run --release
```

При старте приложение пытается загрузить `snake_agent.json`. Если файл найден, эволюция запускается автоматически, используя загруженного агента как семя.

## Как устроено обучение

### Кодирование состояния (зрение + контекст)
Агент «видит» 8 критически важных клеток вокруг головы в относительных координатах (зона 3×3 впереди). Каждая клетка кодируется двумя битами:
- 00 — пусто
- 01 — опасность (стена/тело)
- 10 — яблоко
- 11 — не используется

Это 16 бит. Дополнительно:
- 2 бита: относительное направление до яблока (влево/прямо/вправо)
- 2 бита: категория манхэттенского расстояния до яблока (4 корзины)

Итого: 20 бит (~1 млн состояний).

### Действия
Три дискретных действия относительно текущего направления:
- 0 — повернуть влево
- 1 — прямо
- 2 — повернуть вправо

### Награды
- +10.0 за яблоко, чуть выше с увеличением длины (+0.1 за текущую длину)
- Штраф за смерть зависит от причины: −30.0 за самопересечение (self‑collision); −12.0 в остальных случаях
- Небольшой штраф за шаг (−0.005)
- Shaping: +0.05 при приближении к яблоку и −0.03 при удалении; дополнительно +0.02 при дистанции ≤ 3 клетки

### Параметры QAgent
- Эпсилон‑жадная политика с затуханием (`epsilon`, `min_epsilon`, `decay`)
- Скорость обучения `alpha`, дисконт‑фактор `gamma`
- Счётчики `steps` и `episodes` для метрик

### Эволюционный тренер
- Популяция агентов (по умолчанию 24), каждый играет в своей копии игры
- Предел шагов на эпоху с «защитой лидера»: если один агент лидирует и жив, эпоха не обрывается преждевременно
- В конце эпохи — воспроизводство: элитизм + мутации; многошаговые стратегии рестартов при длительной стагнации с посевом от чемпиона и моментальным автосохранением при улучшении рекорда
- Глобальный «чемпион» (лучший за всё время), автосохранение при улучшении результата
- Визуализация: каждому агенту присваивается уникальный цвет

## Структура кода

- `src/main.rs` — основной бинарник со всей логикой игры/рендера, Q‑обучением и тренером.
- `src/gpu_nn.rs` — экспериментальная заготовка NN (сейчас отключена; фича `gpu-nn` не подключена).
- `src/npu.rs` — опциональный NPU (DirectML/ONNX) инференс (Windows; клавиша K).
- `dqn_agent.safetensors` — веса модели DQN (создаётся при активном DQN по клавише S или при выключении J).

## Советы

- Чтобы начать обучение с нуля, удалите `snake_agent.json` или нажмите E, чтобы включить эволюцию.
- На очень высоких скоростях кадры частично пропускаются, а рисование может быть отключено для максимальной производительности.
- Клавиша G меняет только «бюджет шагов», но не включает вычисления на GPU/NPU. Для DQN используйте J, для NPU — K.
- Размеры сетки/клеток задаются константами вверху `main.rs` и легко настраиваются.

## Лицензия

MIT License — см. файл LICENSE.
