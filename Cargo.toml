[package]
name = "Snake-With-NN-Learning"
version = "0.1.5"
edition = "2024"
license = "MIT"

[features]
# Placeholder features to silence cfgs in code (no deps wired)
gpu-nn = []
gpu-nn-experimental = []
# DQN with Candle on GPU (CUDA) or CPU
dqn-gpu = ["dep:candle-core", "dep:candle-nn"]
# Enable CUDA backend for Candle (requires NVIDIA CUDA Toolkit & NVCC)
dqn-gpu-cuda = ["candle-core/cuda"]
# Render via wgpu directly, bypassing pixels
gpu-render = []
## NPU inference via ONNX Runtime + DirectML (Windows only)
npu-directml = ["dep:ort", "dep:ndarray"]

[dependencies]
pixels = "0.13"
winit = "0.28"
winit_input_helper = "0.14"
rand = { version = "0.8", features = ["small_rng"] }
serde = { version = "1.0", features = ["derive"] }
serde_json = "1.0"
ahash = { version = "0.8", features = ["serde"] }
rayon = "1.10"
wgpu = "0.16"
pollster = "0.3"
bytemuck = { version = "1.14", features = ["derive"] }
anyhow = "1.0"
ndarray = { version = "0.16", optional = true, features = ["std"] }

## (Removed) burn/burn-wgpu experimental deps

# Candle (Rust DL) optional deps for DQN
candle-core = { version = "0.8", optional = true, default-features = false }
candle-nn = { version = "0.8", optional = true, default-features = false }

# ONNX Runtime for NPU/DirectML inference (download prebuilt binaries)
ort = { version = "2.0.0-rc.10", optional = true, default-features = false, features = ["download-binaries", "directml", "ndarray"] }

